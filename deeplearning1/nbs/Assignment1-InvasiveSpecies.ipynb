{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1 - Invasive Species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import utils; reload(utils)\n",
    "from utils import plots\n",
    "\n",
    "# Dataset formatting\n",
    "from os import walk\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'data/invasivespecies/'\n",
    "\n",
    "# NEVER ENABLE THIS WHEN CREATING VALIDATION SET (STEP 1)\n",
    "# path = 'data/invasivespecies/sample/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = path+'models/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Validation set and sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just testing out command line\n",
    "%cd data\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!kg config -g -c 'invasive-species-monitoring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip training folder\n",
    "originals_folder = path+'train_original/'\n",
    "# !7z --help\n",
    "if not os.path.exists(originals_folder):\n",
    "    !7z e {path}train.7z -o{originals_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip to test folder\n",
    "test_folder = path+'test/test/'\n",
    "if not os.path.exists(test_folder):\n",
    "    os.makedirs(test_folder)\n",
    "    !7z e {path}test.7z -o{test_folder}\n",
    "\n",
    "# Create sample test folder\n",
    "_, _, filenames = next(os.walk(test_folder))\n",
    "shuf = np.random.permutation(filenames)\n",
    "sample_test_folder = path+'sample/test/test/'\n",
    "if not os.path.exists(sample_test_folder):\n",
    "    os.makedirs(sample_test_folder)\n",
    "for i in range(30):\n",
    "    shutil.copyfile(test_folder+shuf[i], sample_test_folder+shuf[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip training labels\n",
    "import zipfile\n",
    "def unzip_file(filename, path):\n",
    "    filepath = path+filename\n",
    "    if os.path.isfile(filepath):\n",
    "        zip_ref = zipfile.ZipFile(filepath, 'r')\n",
    "        zip_ref.extractall(path)\n",
    "        zip_ref.close()\n",
    "        print('Unzipping file:', filepath)\n",
    "\n",
    "unzip_file('train_labels.csv.zip', path)\n",
    "unzip_file('sample_submission.csv.zip', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get training labels\n",
    "training_labels_df = pd.read_csv(path + 'train_labels.csv')\n",
    "size = training_labels_df.size\n",
    "num_positives = training_labels_df['invasive'].sum()\n",
    "print('Size:', size)\n",
    "print('Positives: {} Percent: {}'.format(num_positives, num_positives/size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get training files\n",
    "import re\n",
    "\n",
    "_, _, filenames = next(walk(originals_folder))\n",
    "\n",
    "\n",
    "p = re.compile('^([0-9]+).jpg')\n",
    "def find_file_id(filename):\n",
    "    m = p.match(filename)\n",
    "    if m is not None:\n",
    "        return int(m.group(1))\n",
    "    else:\n",
    "        print('Could not regex filename: ', filename)\n",
    "        return -1\n",
    "file_ids = map(find_file_id, filenames)\n",
    "\n",
    "# Merge data into one dataframe:\n",
    "file_df = pd.DataFrame([file_ids, filenames], index=['name', 'file']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Label files and move to labeled folder\n",
    "\n",
    "\n",
    "labeled_folder = path+'labeled/'\n",
    "if not os.path.exists(labeled_folder):\n",
    "    os.makedirs(labeled_folder)\n",
    "    \n",
    "labeled_df = pd.merge(training_labels_df, file_df, on='name')\n",
    "for row in labeled_df.itertuples():\n",
    "    label = 'invasive.' if row[2] == 1 else 'noninvasive.'\n",
    "    file_name = row[3]\n",
    "    new_file = labeled_folder+label+file_name;\n",
    "    shutil.copyfile(originals_folder+file_name, new_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate images into training and validation sets\n",
    "train_folder = path+'train/'\n",
    "shutil.copytree(labeled_folder, train_folder)\n",
    "_, _, filenames = next(walk(train_folder))\n",
    "shuf = np.random.permutation(filenames)\n",
    "size = len(filenames)\n",
    "print(size)\n",
    "\n",
    "valid_folder = path+'valid/'\n",
    "if not os.path.exists(valid_folder):\n",
    "    os.makedirs(valid_folder)\n",
    "    \n",
    "sample_train_folder = path+'sample/train/'\n",
    "if not os.path.exists(sample_train_folder):\n",
    "    os.makedirs(sample_train_folder)\n",
    "for i in range(200):\n",
    "    copyfile(train_folder+shuf[i], sample_train_folder+shuf[i])\n",
    "    \n",
    "sample_valid_folder = path+'sample/valid/'\n",
    "if not os.path.exists(sample_valid_folder):\n",
    "    os.makedirs(sample_valid_folder)\n",
    "for i in range(200):\n",
    "    copyfile(train_folder+shuf[i], sample_valid_folder+shuf[i])\n",
    "    \n",
    "validation_size = int(round(size * .3))\n",
    "for i in range(validation_size):\n",
    "    os.rename(train_folder+shuf[i], valid_folder+shuf[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_folders(folder):\n",
    "    if not os.path.exists(folder+'noninvasive'):\n",
    "        os.makedirs(folder+'noninvasive')\n",
    "        os.makedirs(folder+'invasive')\n",
    "    inv = glob(folder+'invasive.*.jpg')\n",
    "    noninv = glob(folder+'noninvasive.*.jpg')\n",
    "    move_files_to(inv, folder, folder+'invasive/')\n",
    "    move_files_to(noninv, folder, folder+'noninvasive/')\n",
    "    \n",
    "def move_files_to(files, old_folder, new_folder):\n",
    "    for fname in files:\n",
    "        newf = fname.replace(old_folder, new_folder)\n",
    "        os.rename(fname, newf)\n",
    "        \n",
    "classify_folders(train_folder)\n",
    "classify_folders(valid_folder)\n",
    "classify_folders(sample_train_folder)\n",
    "classify_folders(sample_valid_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "# batch_size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vgg16; reload(vgg16)\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = vgg.get_batches(path+'train', batch_size=4)\n",
    "imgs,labels = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(imgs.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imgs[0, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plots(imgs, titles=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.predict(imgs, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.classes[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "from utils import *\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data = get_data(path+'valid')\n",
    "trn_data = get_data(path+'train')\n",
    "save_array(model_path+'train_data.bc', trn_data)\n",
    "save_array(model_path+'valid_data.bc', val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_data = load_array(model_path+'train_data.bc')\n",
    "val_data = load_array(model_path+'valid_data.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1167 images belonging to 2 classes.\n",
      "Found 1606 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "batches = get_batches(path+'train', shuffle=False, batch_size=1)\n",
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())\n",
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the class ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valuation distribution:  [ 752.  415.]\n",
      "Percentages:  [ 0.6444  0.3556]\n",
      "Training distribution:  [ 995.  611.]\n",
      "Percentages:  [ 0.6196  0.3804]\n"
     ]
    }
   ],
   "source": [
    "print('Valuation distribution: ', sum(val_labels))\n",
    "print('Percentages: ', sum(val_labels) / val_labels.shape[0])\n",
    "\n",
    "print('Training distribution: ', sum(trn_labels))\n",
    "print('Percentages: ', sum(trn_labels) / trn_labels.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Auto Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_vgg(save_file='trained_weights.h5', nb_epoch=1):\n",
    "    batch_size=64\n",
    "    batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "    val_batches = vgg.get_batches(path+'valid', batch_size=batch_size*2)\n",
    "    vgg.fit(batches, val_batches, nb_epoch=nb_epoch)\n",
    "    vgg.model.save_weights(model_path + save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1606 images belonging to 2 classes.\n",
      "Found 1167 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 129s - loss: 0.6809 - acc: 0.7640 - val_loss: 0.2942 - val_acc: 0.8800\n"
     ]
    }
   ],
   "source": [
    "vgg.finetune(batches)\n",
    "\n",
    "\n",
    "train_vgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1606 images belonging to 2 classes.\n",
      "Found 1167 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 111s - loss: 0.4358 - acc: 0.8493 - val_loss: 0.2457 - val_acc: 0.8972\n"
     ]
    }
   ],
   "source": [
    "train_vgg('trained_weights_n2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1606 images belonging to 2 classes.\n",
      "Found 1167 images belonging to 2 classes.\n",
      "Epoch 1/1\n",
      "1606/1606 [==============================] - 111s - loss: 0.4096 - acc: 0.8580 - val_loss: 0.2159 - val_acc: 0.9100\n"
     ]
    }
   ],
   "source": [
    "train_vgg('trained_weights_n3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights(model_path + 'trained_weights_n2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1606 images belonging to 2 classes.\n",
      "Found 1167 images belonging to 2 classes.\n",
      "Epoch 1/2\n",
      "1606/1606 [==============================] - 108s - loss: 0.3470 - acc: 0.8705 - val_loss: 0.2052 - val_acc: 0.9160\n",
      "Epoch 2/2\n",
      "1606/1606 [==============================] - 94s - loss: 0.3762 - acc: 0.8680 - val_loss: 0.2205 - val_acc: 0.9117\n"
     ]
    }
   ],
   "source": [
    "train_vgg('trained_weights_n4.h5', nb_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from Cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size*2)\n",
    "vgg.finetune(batches)\n",
    "\n",
    "if os.path.exists(weights_fullpath):\n",
    "    vgg.model.load_weights(weights_fullpath)\n",
    "    print('Loaded model from cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Option 2: Manual fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen=image.ImageDataGenerator()\n",
    "batches = gen.flow(trn_data, trn_labels, batch_size=batch_size, shuffle=True)\n",
    "val_batches = gen.flow(val_data, val_labels, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, batches, val_batches, nb_epoch=1):\n",
    "    model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=nb_epoch,\n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1606/1606 [==============================] - 147s - loss: 5.9369 - acc: 0.6102 - val_loss: 5.7318 - val_acc: 0.6444\n",
      "Epoch 2/2\n",
      "1606/1606 [==============================] - 147s - loss: 6.1321 - acc: 0.6196 - val_loss: 5.7318 - val_acc: 0.6444\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, batches, val_batches, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'manual_finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'manual_finetune1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning previous layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = vgg.model\n",
    "layers = model.layers\n",
    "# Get the index of the first dense layer...\n",
    "first_dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "# ...and set this and all subsequent layers to trainable\n",
    "for layer in layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1606/1606 [==============================] - 67s - loss: 0.3257 - acc: 0.8755 - val_loss: 0.1866 - val_acc: 0.9289\n",
      "Epoch 2/3\n",
      "1606/1606 [==============================] - 68s - loss: 0.3051 - acc: 0.8823 - val_loss: 0.2060 - val_acc: 0.9117\n",
      "Epoch 3/3\n",
      "1606/1606 [==============================] - 68s - loss: 0.2899 - acc: 0.8842 - val_loss: 0.1900 - val_acc: 0.9229\n"
     ]
    }
   ],
   "source": [
    "K.set_value(opt.lr, 0.01)\n",
    "fit_model(model, batches, val_batches, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'manual_finetune_dense_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in layers[12:]: layer.trainable=True\n",
    "K.set_value(opt.lr, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1606/1606 [==============================] - 68s - loss: 0.2427 - acc: 0.9060 - val_loss: 0.1930 - val_acc: 0.9246\n",
      "Epoch 2/4\n",
      "1606/1606 [==============================] - 67s - loss: 0.3092 - acc: 0.8798 - val_loss: 0.2559 - val_acc: 0.9057\n",
      "Epoch 3/4\n",
      "1606/1606 [==============================] - 67s - loss: 0.2896 - acc: 0.8929 - val_loss: 0.1989 - val_acc: 0.9186\n",
      "Epoch 4/4\n",
      "1606/1606 [==============================] - 67s - loss: 0.2446 - acc: 0.9091 - val_loss: 0.1697 - val_acc: 0.9280\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'manual_finetune_12th_layer_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 28s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.7317992098431656, 0.64438731790916881]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 28s    \n",
      "1167/1167 [==============================] - 28s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = vgg.model.predict_classes(val_data, batch_size=batch_size)\n",
    "# probs = vgg.model.predict_proba(val_data, batch_size=batch_size)[:,0]\n",
    "# probs[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[752   0]\n",
      " [415   0]]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "cm = confusion_matrix(val_classes, preds)\n",
    "print(cm)\n",
    "print(sklearn.metrics.f1_score(val_classes, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm, {'noninvasive':0, 'invasive':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict results in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531\n",
      "Found 1531 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "testFolder = path+'test/'\n",
    "\n",
    "# another test directory embeded in test folder\n",
    "embeddedFolder = testFolder+'test/'\n",
    "_, _, files = os.walk(embeddedFolder).next()\n",
    "num_test_images = len(files)\n",
    "test_batch_size = 100\n",
    "\n",
    "print(num_test_images)\n",
    "# num_test_images = 8\n",
    "# test_batch_size = 1\n",
    "\n",
    "batches = vgg.get_batches(testFolder, batch_size=test_batch_size, shuffle=False, class_mode=None)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filenames = batches.filenames\n",
    "filename = filenames[0]\n",
    "print(filename)\n",
    "\n",
    "p = re.compile('.*/([0-9]+).jpg')\n",
    "m = p.match(filename)\n",
    "m.group(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our predictions..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing batch results\n",
    "test_batch_size = 10\n",
    "# test_sample_folder = path+'sample/test/'\n",
    "\n",
    "batches = vgg.get_batches(testFolder, batch_size=test_batch_size, shuffle=False, class_mode=None)\n",
    "# batches = vgg.get_batches(test_sample_folder, batch_size=test_batch_size, shuffle=False, class_mode=None)\n",
    "imgs = next(batches)\n",
    "labels, a, b = vgg.predict(imgs, True)\n",
    "\n",
    "# This shows the 'ground truth'\n",
    "plots(imgs, titles=labels)\n",
    "\n",
    "p = vgg.model.predict_on_batch(imgs)\n",
    "print(p)\n",
    "print(labels)\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to convert file names to label ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "filenames = batches.filenames\n",
    "\n",
    "p = re.compile('.*/([0-9]+).jpg')\n",
    "def find_file_id(filename):\n",
    "    m = p.match(filename)\n",
    "    if m is not None:\n",
    "        return int(m.group(1))\n",
    "    else:\n",
    "        print('Could not regex filename: ', filename)\n",
    "        return -1\n",
    "file_ids = map(find_file_id, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option A: Predict values using vgg test function (Not recommended. No progress)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing batch results\n",
    "batches, predictions = vgg.test(testFolder, batch_size=1)\n",
    "# print('A: ', batches)\n",
    "p_results = predictions[:, 1]\n",
    "\n",
    "print('Predictions: ', p_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option B: Predict values with batches and progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "  0%|          | 0/16.0 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1531 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "\n",
      "  6%|▋         | 1/16.0 [00:05<01:27,  5.81s/it]\u001b[A\n",
      " 12%|█▎        | 2/16.0 [00:11<01:21,  5.81s/it]\u001b[A\n",
      " 19%|█▉        | 3/16.0 [00:17<01:15,  5.77s/it]\u001b[A\n",
      " 25%|██▌       | 4/16.0 [00:23<01:09,  5.77s/it]\u001b[A\n",
      " 31%|███▏      | 5/16.0 [00:28<01:03,  5.76s/it]\u001b[A\n",
      " 38%|███▊      | 6/16.0 [00:34<00:57,  5.74s/it]\u001b[A\n",
      " 44%|████▍     | 7/16.0 [00:40<00:51,  5.74s/it]\u001b[A\n",
      " 50%|█████     | 8/16.0 [00:45<00:45,  5.73s/it]\u001b[A\n",
      " 56%|█████▋    | 9/16.0 [00:51<00:40,  5.72s/it]\u001b[A\n",
      " 62%|██████▎   | 10/16.0 [00:57<00:34,  5.73s/it]\u001b[A\n",
      " 69%|██████▉   | 11/16.0 [01:03<00:28,  5.75s/it]\u001b[A\n",
      " 75%|███████▌  | 12/16.0 [01:08<00:22,  5.75s/it]\u001b[A\n",
      " 81%|████████▏ | 13/16.0 [01:14<00:17,  5.76s/it]\u001b[A\n",
      " 88%|████████▊ | 14/16.0 [01:20<00:11,  5.75s/it]\u001b[A\n",
      " 94%|█████████▍| 15/16.0 [01:26<00:05,  5.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.6409e-04   9.7042e-01   9.7979e-01 ...,   9.9461e-01   9.7114e-01   9.9493e-01]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "predict_file = model_path + 'predict.bc'\n",
    "\n",
    "def predict_test():\n",
    "    batches = vgg.get_batches(testFolder, batch_size=test_batch_size, shuffle=False, class_mode=None)\n",
    "\n",
    "    p_results = np.zeros(num_test_images)\n",
    "    current_index = 0\n",
    "    # Iterative loop\n",
    "    for batch in tqdm(batches, total=math.ceil(num_test_images/test_batch_size)):\n",
    "        if batch is None:\n",
    "            break\n",
    "        p = vgg.model.predict_on_batch(batch)\n",
    "        p_true = p[:, 1]\n",
    "        p_size = p.shape[0]\n",
    "#         print('Predictions: {}\\n Size: {}'.format(p_true, p_size))\n",
    "        new_index = current_index + p_size\n",
    "#         print('Current index: {} New index: {} PResults: {}'.format(current_index, new_index, p_results))\n",
    "        p_results[current_index:new_index] = p_true\n",
    "        current_index = new_index\n",
    "        if current_index >= num_test_images:\n",
    "            break\n",
    "    print(p_results)\n",
    "    utils.save_array(predict_file, p_results)\n",
    "    return p_results\n",
    "\n",
    "if os.path.exists(predict_file):\n",
    "    p_results = utils.load_array(predict_file)\n",
    "    print('Loaded predictions from cache')\n",
    "else:\n",
    "    p_results = predict_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1531,)\n",
      "1531\n",
      "[  7.6409e-04   9.7042e-01   9.7979e-01 ...,   9.9461e-01   9.7114e-01   9.9493e-01]\n"
     ]
    }
   ],
   "source": [
    "# Verify the arrays match\n",
    "print(p_results.shape)\n",
    "print(len(file_ids))\n",
    "print(p_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rounded_results = np.rint(p_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clip results for better log loss\n",
    "clipped_results = np.clip(p_results, 0.05, 0.95)\n",
    "\n",
    "clipped_file = model_path + 'clip.bc'\n",
    "utils.save_array(clipped_file, clipped_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load saved clip file\n",
    "clipped_file = models_folder + 'clip.bc'\n",
    "clipped_results = load_array(clipped_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine ids with labels and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name  invasive\n",
      "865      1  0.244312\n",
      "487      2  0.950000\n",
      "432      3  0.924252\n",
      "688      4  0.602737\n",
      "334      5  0.099905\n",
      "468      6  0.950000\n",
      "1163     7  0.661849\n",
      "1017     8  0.050000\n",
      "1135     9  0.050000\n",
      "774     10  0.950000\n",
      "295     11  0.950000\n",
      "900     12  0.950000\n",
      "1460    13  0.950000\n",
      "250     14  0.050000\n",
      "1206    15  0.642296\n",
      "657     16  0.657809\n",
      "164     17  0.950000\n",
      "20      18  0.324960\n",
      "1482    19  0.458225\n",
      "1088    20  0.850550\n",
      "837     21  0.050000\n",
      "242     22  0.950000\n",
      "178     23  0.050000\n",
      "215     24  0.704051\n",
      "411     25  0.050000\n",
      "1242    26  0.050000\n",
      "95      27  0.050000\n",
      "605     28  0.950000\n",
      "1307    29  0.950000\n",
      "1479    30  0.950000\n",
      "...    ...       ...\n",
      "246   1502  0.946256\n",
      "1378  1503  0.950000\n",
      "744   1504  0.950000\n",
      "1210  1505  0.050000\n",
      "554   1506  0.050000\n",
      "1423  1507  0.769502\n",
      "1363  1508  0.950000\n",
      "1021  1509  0.050000\n",
      "1051  1510  0.050000\n",
      "878   1511  0.050203\n",
      "420   1512  0.884837\n",
      "1386  1513  0.879919\n",
      "988   1514  0.050000\n",
      "922   1515  0.950000\n",
      "1179  1516  0.050000\n",
      "193   1517  0.050000\n",
      "476   1518  0.185062\n",
      "465   1519  0.933922\n",
      "402   1520  0.950000\n",
      "1134  1521  0.910990\n",
      "1430  1522  0.950000\n",
      "1327  1523  0.050000\n",
      "1043  1524  0.368777\n",
      "630   1525  0.077038\n",
      "1074  1526  0.802231\n",
      "409   1527  0.950000\n",
      "902   1528  0.050000\n",
      "665   1529  0.354163\n",
      "923   1530  0.050000\n",
      "1337  1531  0.050000\n",
      "\n",
      "[1531 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "agg = pd.DataFrame({'name': file_ids, 'invasive': clipped_results})\n",
    "agg = agg[agg.columns[::-1]]\n",
    "# agg = pd.DataFrame([file_ids, clipped_results], columns=['name', 'invasive'])\n",
    "agg = agg.sort_values(['name'])\n",
    "print(agg)\n",
    "agg.to_csv(path + 'clipped.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(path+'clipped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!kg config -g -c 'invasive-species-monitoring'\n",
    "!kg submit {path+'clipped.csv'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing VGG with Keras Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FILES_PATH = 'http://files.fast.ai/models/'\n",
    "CLASS_FILE='imagenet_class_index.json'\n",
    "fpath = get_file(CLASS_FILE, FILES_PATH+CLASS_FILE, cache_subdir='models')\n",
    "with open(fpath) as f:\n",
    "    class_dict = json.load(f)\n",
    "# classes = [class_dict[i][1] for i in class_dict]\n",
    "classes = [class_dict[str(i)][1] for i in range(len(class_dict))] # original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ConvBlock(layers, model, filters):\n",
    "    for i in range(layers): \n",
    "        model.add(ZeroPadding2D((1,1)))\n",
    "        model.add(Convolution2D(filters, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FCBlock(model):\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ??Convolution2D\n",
    "\n",
    "# ??MaxPooling2D\n",
    "\n",
    "# ??Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mean of each channel as provided by VGG researchers\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((3,1,1))\n",
    "\n",
    "def vgg_preprocess(x):\n",
    "    x = x - vgg_mean     # subtract mean\n",
    "    return x[:, ::-1]    # reverse axis bgr->rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGG_16():\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(vgg_preprocess, input_shape=(3,224,224)))\n",
    "\n",
    "    ConvBlock(2, model, 64)\n",
    "    ConvBlock(2, model, 128)\n",
    "    ConvBlock(3, model, 256)\n",
    "    ConvBlock(3, model, 512)\n",
    "    ConvBlock(3, model, 512)\n",
    "\n",
    "    model.add(Flatten())\n",
    "    FCBlock(model)\n",
    "    FCBlock(model)\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??MaxPooling2D\n",
    "\n",
    "??Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = VGG_16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpath = get_file('vgg16.h5', FILES_PATH+'vgg16.h5', cache_subdir='models')\n",
    "model.load_weights(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, \n",
    "                batch_size=batch_size, class_mode='categorical'):\n",
    "    return gen.flow_from_directory(path+dirname, target_size=(224,224), \n",
    "                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches('train', batch_size=batch_size)\n",
    "val_batches = get_batches('valid', batch_size=batch_size)\n",
    "imgs,labels = next(batches)\n",
    "\n",
    "# This shows the 'ground truth'\n",
    "plots(imgs, titles=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred_batch(imgs):\n",
    "    preds = model.predict(imgs)\n",
    "    idxs = np.argmax(preds, axis=1)\n",
    "\n",
    "    print('Shape: {}'.format(preds.shape))\n",
    "    print('First 5 classes: {}'.format(classes[:5]))\n",
    "    print('First 5 probabilities: {}\\n'.format(preds[0, :5]))\n",
    "    print('Predictions prob/class: ')\n",
    "    \n",
    "    for i in range(len(idxs)):\n",
    "        idx = idxs[i]\n",
    "        print ('  {:.4f}/{}'.format(preds[i, idx], classes[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_batch(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
