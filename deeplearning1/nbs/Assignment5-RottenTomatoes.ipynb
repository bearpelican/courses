{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is enabled with initial size: 80.0% of memory, cuDNN 5103)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'data/rt/'\n",
    "sample_path = 'data/rt/sample/'\n",
    "path = data_path\n",
    "model_path = path + 'models/'\n",
    "results_path = path + 'results/'\n",
    "home_dir = os.path.realpath('.')\n",
    "for p in [path, model_path, sample_path, results_path]:\n",
    "    if not os.path.exists(p):\n",
    "        os.makedirs(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Unzip Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download\n",
    "%cd {path}\n",
    "!kg download -c 'sentiment-analysis-on-movie-reviews'\n",
    "%cd {home_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip training labels\n",
    "import zipfile\n",
    "def unzip_file(filename, path, outputpath=None):\n",
    "    outputpath = outputpath or path\n",
    "    filepath = path + filename\n",
    "    if not os.path.exists(outputpath):\n",
    "        os.makedirs(outputpath)\n",
    "    if os.path.isfile(filepath):\n",
    "        zip_ref = zipfile.ZipFile(filepath, 'r')\n",
    "        zip_ref.extractall(outputpath)\n",
    "        zip_ref.close()\n",
    "        print('Unzipping file:', filepath)\n",
    "\n",
    "unzip_file('test.tsv.zip', path)\n",
    "unzip_file('train.tsv.zip', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset - index words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(path+'train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(reviews.shape)\n",
    "reviews[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "full_phrase_idx = reviews.groupby('SentenceId')['PhraseId'].min()\n",
    "# print(full_phrase_idx.values)\n",
    "full_phrases_df = reviews.loc[reviews['PhraseId'].isin(full_phrase_idx.values)]\n",
    "print(type(full_phrases_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .',\n",
       " 'a series of escapades demonstrating the adage that what is good for the goose',\n",
       " 'a series',\n",
       " 'a',\n",
       " 'series']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df = full_phrases_df.copy()\n",
    "train_df = reviews.copy()\n",
    "\n",
    "train_phrases = train_df['Phrase']\n",
    "norm_phrases = map(str.lower, train_phrases.tolist())\n",
    "\n",
    "norm_phrases[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future improvements:\n",
    "\n",
    "Use keras.preprocessing.text.Tokenizer instead of manually tokenizing everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def sorted_wordlist(phrases):\n",
    "    sentencelist = map(str.split, phrases)\n",
    "    flat_list = [word for sublist in sentencelist for word in sublist]\n",
    "    wordcounts = Counter(flat_list)\n",
    "    print('Words:', len(wordcounts))\n",
    "    wordlist = [x[0] for x in wordcounts.most_common()]\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: 16531\n"
     ]
    }
   ],
   "source": [
    "wordlist = sorted_wordlist(norm_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16531"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index should start at 1. 0 is for padding\n",
    "word2idx = {word: idx+1 for idx, word in enumerate(wordlist)}\n",
    "idx2word = {idx+1: word for idx, word in enumerate(wordlist)}\n",
    "len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def map_sentence2idx(sentences, word2idx):\n",
    "    return map(partial(sentence2idx, word2idx), sentences)\n",
    "# Map each sentence to phrases\n",
    "def sentence2idx(wordMap, sentence):\n",
    "    words = sentence.split()\n",
    "    def map_word2idx(word):\n",
    "        if word in wordMap:\n",
    "            return wordMap[word]\n",
    "        return len(wordMap)\n",
    "    return map(map_word2idx, words)\n",
    "\n",
    "idx_sentencelist = map_sentence2idx(norm_phrases, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 0, 7.2033640907343326)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at distribution of lengths of sentences\n",
    "lens = np.array(map(len, idx_sentencelist))\n",
    "(lens.max(), lens.min(), lens.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_idx(wordidx, vocab_size=8000, seq_len=55):\n",
    "    limit_idx = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in wordidx]\n",
    "#     print(len(limit_idx))\n",
    "    padded_idx = sequence.pad_sequences(limit_idx, maxlen=seq_len, value=0)\n",
    "    return [np.array(x) for x in padded_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Limit vocab size\n",
    "vocab_size = 8000\n",
    "# Pad (with zero) or truncate to max sentence length\n",
    "seq_len = 55\n",
    "\n",
    "normalized_word_idx = norm_idx(idx_sentencelist, vocab_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['word_idx'] = pd.Series(normalized_word_idx, index=train_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>word_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "5         6           1  of escapades demonstrating the adage that what...   \n",
       "6         7           1                                                 of   \n",
       "7         8           1  escapades demonstrating the adage that what is...   \n",
       "8         9           1                                          escapades   \n",
       "9        10           1  demonstrating the adage that what is good for ...   \n",
       "\n",
       "   Sentiment                                           word_idx  \n",
       "0          1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1          2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2          2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3          2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4          2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5          2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6          2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7          2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "8          2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "9          2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(results_path+'train_idx.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(train_df, results_path+'train_idx.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(train_df.SentenceId.unique())\n",
    "train_sentence_ids = perm[:int(len(perm)*.8)]\n",
    "train = train_df.loc[train_df.SentenceId.isin(train_sentence_ids)]\n",
    "train = train.sample(frac=1)\n",
    "test = shuffle.loc[~train_df.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124885, 5), (31175, 5))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# OLD WAY - SENTENCES WERE MIXED IN TRAIN AND TEST\n",
    "shuffle = train_df.sample(frac=1)\n",
    "train_df.SentenceId.unique\n",
    "train = shuffle.sample(frac=0.8)\n",
    "test = shuffle.loc[~shuffle.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train.word_idx.tolist())\n",
    "labels_train = onehot(np.array(train.Sentiment.tolist()))\n",
    "x_test = np.array(test.word_idx.tolist())\n",
    "labels_test = onehot(np.array(test.Sentiment.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124885, 55), (124885, 5))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train.shape, labels_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cached Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = get_data(path+'train')\n",
    "val = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(results_path+'val.dat', val)\n",
    "save_array(results_path+'trn.dat', trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = load_array(results_path+'val.dat')\n",
    "trn = load_array(results_path+'trn.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(labels_train.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, labels_train, validation_data=(x_test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=32, input_length=seq_len, dropout=0.2),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(nb_filter=32, filter_length=5, border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(labels_train.shape[1], activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, labels_train, validation_data=(x_test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit(x_train, labels_train, validation_data=(x_test, labels_test), nb_epoch=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove_dataset(dataset):\n",
    "    \"\"\"Download the requested glove dataset from files.fast.ai\n",
    "    and return a location that can be passed to load_vectors.\n",
    "    \"\"\"\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://files.fast.ai/models/glove/6B.100d.tgz\n",
      "Untaring file...\n"
     ]
    }
   ],
   "source": [
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.100d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        word = idx2word[i]\n",
    "        if word in wordidx and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            print('Could not find word in glove:', word)\n",
    "            emb[i] = normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.05,\n",
    "              weights=[emb], trainable=False),\n",
    "    BatchNormalization(),\n",
    "#     Dropout(0.15),\n",
    "    Convolution1D(64, 6, border_mode='same', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "#     Dropout(0.15),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "#     Dropout(0.3),\n",
    "    Dense(5, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, labels_train, validation_data=(x_test, labels_test), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.01\n",
    "model.fit(x_train, labels_train, validation_data=(x_test, labels_test), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train first layer\n",
    "model.layers[0].trainable = True\n",
    "model.optimizer.lr = 1e-4\n",
    "model.fit(x_train, labels_train, validation_data=(x_test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multi-convnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_factors = 100\n",
    "graph_in = Input((vocab_size, latent_factors))\n",
    "convs = [ ]\n",
    "for fsz in range(2, 6):\n",
    "    x = Convolution1D(100, fsz, border_mode='same', activation='relu')(graph_in)\n",
    "    x = MaxPooling1D(pool_length=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    convs.append(x)\n",
    "# out = keras.layers.Merge(convs, mode='concat')\n",
    "out = merge(convs, mode='concat')\n",
    "\n",
    "graph = Model(graph_in, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find word in glove: ,\n",
      "Could not find word in glove: .\n",
      "Could not find word in glove: 's\n",
      "Could not find word in glove: n't\n",
      "Could not find word in glove: '\n",
      "Could not find word in glove: -rrb-\n",
      "Could not find word in glove: -lrb-\n",
      "Could not find word in glove: `\n",
      "Could not find word in glove: ...\n",
      "Could not find word in glove: ``\n",
      "Could not find word in glove: ''\n",
      "Could not find word in glove: 're\n",
      "Could not find word in glove: :\n",
      "Could not find word in glove: 've\n",
      "Could not find word in glove: 'll\n",
      "Could not find word in glove: ;\n",
      "Could not find word in glove: ?\n",
      "Could not find word in glove: mr.\n",
      "Could not find word in glove: 'd\n",
      "Could not find word in glove: !\n",
      "Could not find word in glove: 'm\n",
      "Could not find word in glove: ms.\n",
      "Could not find word in glove: &\n",
      "Could not find word in glove: vs.\n",
      "Could not find word in glove: $\n",
      "Could not find word in glove: 'em\n",
      "Could not find word in glove: '70s\n",
      "Could not find word in glove: \\/\n",
      "Could not find word in glove: j.\n",
      "Could not find word in glove: '60s\n",
      "Could not find word in glove: \\*\\*\\*\n",
      "Could not find word in glove: writer\\/director\n",
      "Could not find word in glove: dumbed-down\n",
      "Could not find word in glove: no.\n",
      "Could not find word in glove: and\\/or\n",
      "Could not find word in glove: slo-mo\n",
      "Could not find word in glove: picture-perfect\n",
      "Could not find word in glove: birot\n",
      "Could not find word in glove: '50s\n",
      "Could not find word in glove: h.g.\n",
      "Could not find word in glove: prep-school\n",
      "Could not find word in glove: bros.\n",
      "Could not find word in glove: wollter\n",
      "Could not find word in glove: jr.\n",
      "Could not find word in glove: kosashvili\n",
      "Could not find word in glove: mother\\/daughter\n",
      "Could not find word in glove: e.t.\n",
      "Could not find word in glove: \\*\n",
      "Could not find word in glove: etc.\n",
      "Could not find word in glove: cletis\n",
      "Could not find word in glove: feardotcom\n",
      "Could not find word in glove: mind-numbingly\n",
      "Could not find word in glove: m.\n",
      "Could not find word in glove: ham-fisted\n",
      "Could not find word in glove: x.\n",
      "Could not find word in glove: stomach-turning\n",
      "Could not find word in glove: movie-star\n",
      "Could not find word in glove: punch-drunk\n",
      "Could not find word in glove: 1\\/2\n",
      "Could not find word in glove: connect-the-dots\n",
      "Could not find word in glove: waydowntown\n",
      "Could not find word in glove: seldahl\n",
      "Could not find word in glove: hour-and-a-half\n",
      "Could not find word in glove: l.a.\n",
      "Could not find word in glove: back-stabbing\n",
      "Could not find word in glove: ol'\n",
      "Could not find word in glove: paint-by-number\n",
      "Could not find word in glove: intacto\n",
      "Could not find word in glove: inc.\n",
      "Could not find word in glove: cheese-laced\n",
      "Could not find word in glove: self-caricature\n",
      "Could not find word in glove: qatsi\n",
      "Could not find word in glove: pile-ups\n",
      "Could not find word in glove: guilty-pleasure\n",
      "Could not find word in glove: r.\n",
      "Could not find word in glove: c.\n",
      "Could not find word in glove: nebrida\n",
      "Could not find word in glove: ?!?\n",
      "Could not find word in glove: dr.\n",
      "Could not find word in glove: not-very-funny\n",
      "Could not find word in glove: idemoto\n",
      "Could not find word in glove: all-too-familiar\n",
      "Could not find word in glove: dime-store\n",
      "Could not find word in glove: al.\n",
      "Could not find word in glove: damaged-goods\n",
      "Could not find word in glove: 9\\/11\n",
      "Could not find word in glove: world-at-large\n",
      "Could not find word in glove: bargain-basement\n",
      "Could not find word in glove: codswallop\n",
      "Could not find word in glove: fish-out-of-water\n",
      "Could not find word in glove: janklowicz-mann\n",
      "Could not find word in glove: pre-credit\n",
      "Could not find word in glove: blood-curdling\n",
      "Could not find word in glove: been-there\n",
      "Could not find word in glove: toilet-humor\n",
      "Could not find word in glove: so-bad-it\n",
      "Could not find word in glove: multi-character\n",
      "Could not find word in glove: strung-together\n",
      "Could not find word in glove: nickelodeon-esque\n",
      "Could not find word in glove: water-bound\n",
      "Could not find word in glove: old-hat\n",
      "Could not find word in glove: #\n",
      "Could not find word in glove: clung-to\n",
      "Could not find word in glove: doing-it-for\n",
      "Could not find word in glove: slap-happy\n",
      "Could not find word in glove: ming-liang\n",
      "Could not find word in glove: half-asleep\n",
      "Could not find word in glove: crime-land\n",
      "Could not find word in glove: flesh-and-blood\n",
      "Could not find word in glove: frissons\n",
      "Could not find word in glove: anti-kieslowski\n",
      "Could not find word in glove: right-thinking\n",
      "Could not find word in glove: kalvert\n",
      "Could not find word in glove: shrieky\n",
      "Could not find word in glove: pop-induced\n",
      "Could not find word in glove: blank-faced\n",
      "Could not find word in glove: tear-jerking\n",
      "Could not find word in glove: young-guns\n",
      "Could not find word in glove: shmear\n",
      "Could not find word in glove: oh-so\n",
      "Could not find word in glove: transfigures\n",
      "Could not find word in glove: teen-driven\n",
      "Could not find word in glove: indie-heads\n",
      "Could not find word in glove: fever-pitched\n",
      "Could not find word in glove: rah-rah\n",
      "Could not find word in glove: opera-ish\n",
      "Could not find word in glove: girl-meets-girl\n",
      "Could not find word in glove: w.\n",
      "Could not find word in glove: teendom\n",
      "Could not find word in glove: self-promoter\n",
      "Could not find word in glove: fleshed-out\n",
      "Could not find word in glove: jean-claud\n",
      "Could not find word in glove: the-cash\n",
      "Could not find word in glove: fun-for-fun\n",
      "Could not find word in glove: truncheoning\n",
      "Could not find word in glove: through-line\n",
      "Could not find word in glove: smart-aleck\n",
      "Could not find word in glove: auteil\n",
      "Could not find word in glove: monster-in-the\n",
      "Could not find word in glove: chopsocky\n",
      "Could not find word in glove: 1.8\n",
      "Could not find word in glove: girl-on-girl\n",
      "Could not find word in glove: self-glorification\n",
      "Could not find word in glove: still-inestimable\n",
      "Could not find word in glove: sequel-for-the-sake\n",
      "Could not find word in glove: flatula\n",
      "Could not find word in glove: screwed-up\n",
      "Could not find word in glove: shapable\n",
      "Could not find word in glove: genial-rogue\n",
      "Could not find word in glove: heart-breakingly\n",
      "Could not find word in glove: self-empowering\n",
      "Could not find word in glove: power-lunchers\n",
      "Could not find word in glove: guy-in-a-dress\n",
      "Could not find word in glove: ozpetek\n",
      "Could not find word in glove: hopped-up\n",
      "Could not find word in glove: handbag-clutching\n",
      "Could not find word in glove: second-guess\n",
      "Could not find word in glove: u.s.\n",
      "Could not find word in glove: by-the-numbers\n",
      "Could not find word in glove: '30s\n",
      "Could not find word in glove: boundary-hopping\n",
      "Could not find word in glove: summer-camp\n",
      "Could not find word in glove: brothers\\/abrahams\n",
      "Could not find word in glove: siuation\n",
      "Could not find word in glove: s\\/m\n",
      "Could not find word in glove: white-on-black\n",
      "Could not find word in glove: 102-minute\n",
      "Could not find word in glove: lip-non-synching\n",
      "Could not find word in glove: chou-chou\n",
      "Could not find word in glove: all-enveloping\n",
      "Could not find word in glove: ages-old\n",
      "Could not find word in glove: age-inspired\n",
      "Could not find word in glove: water-born\n",
      "Could not find word in glove: overly-familiar\n",
      "Could not find word in glove: well-shaped\n",
      "Could not find word in glove: u.n.\n",
      "Could not find word in glove: now-cliched\n",
      "Could not find word in glove: hole-ridden\n",
      "Could not find word in glove: ho-tep\n",
      "Could not find word in glove: disappearing\\/reappearing\n",
      "Could not find word in glove: laissez-passer\n",
      "Could not find word in glove: vidgame\n",
      "Could not find word in glove: razzle-dazzle\n",
      "Could not find word in glove: paint-by-numbers\n",
      "Could not find word in glove: heart-pounding\n",
      "Could not find word in glove: rose-tinted\n",
      "Could not find word in glove: channel-style\n",
      "Could not find word in glove: chick-flicks\n",
      "Could not find word in glove: 103-minute\n",
      "Could not find word in glove: pseudo-educational\n",
      "Could not find word in glove: manipulativeness\n",
      "Could not find word in glove: car-wreck\n",
      "Could not find word in glove: ballistic-pyrotechnic\n",
      "Could not find word in glove: head-trip\n",
      "Could not find word in glove: shoot-em-up\n",
      "Could not find word in glove: tongue-tied\n",
      "Could not find word in glove: unhidden\n",
      "Could not find word in glove: o'fallon\n",
      "Could not find word in glove: human-scale\n",
      "Could not find word in glove: campaign-trail\n",
      "Could not find word in glove: '40s\n",
      "Could not find word in glove: q.\n",
      "Could not find word in glove: rocky-like\n",
      "Could not find word in glove: kids-and-family-oriented\n",
      "Could not find word in glove: hardass\n",
      "Could not find word in glove: s.c.\n",
      "Could not find word in glove: glass-shattering\n",
      "Could not find word in glove: self-mutilating\n",
      "Could not find word in glove: corniest\n",
      "Could not find word in glove: post-feminist\n",
      "Could not find word in glove: unamusing\n",
      "Could not find word in glove: bad-movie\n",
      "Could not find word in glove: over-blown\n",
      "Could not find word in glove: big-wave\n",
      "Could not find word in glove: lip-gloss\n",
      "Could not find word in glove: stadium-seat\n",
      "Could not find word in glove: fast-edit\n",
      "Could not find word in glove: hungry-man\n",
      "Could not find word in glove: soon-to-be-forgettable\n",
      "Could not find word in glove: dead-undead\n",
      "Could not find word in glove: amusedly\n",
      "Could not find word in glove: raunch-fests\n",
      "Could not find word in glove: of-a-sequel\n",
      "Could not find word in glove: light-footed\n",
      "Could not find word in glove: cliff-notes\n",
      "Could not find word in glove: movie-of-the-week\n",
      "Could not find word in glove: half-sleep\n",
      "Could not find word in glove: pokepie\n",
      "Could not find word in glove: all-too-human\n",
      "Could not find word in glove: she-cute\n",
      "Could not find word in glove: nature\\/nurture\n",
      "Could not find word in glove: post-adolescent\n",
      "Could not find word in glove: belly-dancing\n",
      "Could not find word in glove: well-lensed\n",
      "Could not find word in glove: jaw-droppingly\n",
      "Could not find word in glove: out-bad-act\n",
      "Could not find word in glove: tries-so-hard-to-be-cool\n"
     ]
    }
   ],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, latent_factors, input_length=seq_len, dropout=.2, weights=[emb]),\n",
    "#     BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    graph,\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    Dense(100, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "    Dense(70, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.15),\n",
    "    Dense(5, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124885 samples, validate on 31175 samples\n",
      "Epoch 1/5\n",
      "124885/124885 [==============================] - 31s - loss: 1.0588 - acc: 0.5635 - val_loss: 0.8858 - val_acc: 0.6279\n",
      "Epoch 2/5\n",
      "124885/124885 [==============================] - 31s - loss: 0.9430 - acc: 0.6141 - val_loss: 0.8342 - val_acc: 0.6544\n",
      "Epoch 3/5\n",
      "124885/124885 [==============================] - 31s - loss: 0.8983 - acc: 0.6300 - val_loss: 0.8199 - val_acc: 0.6548\n",
      "Epoch 4/5\n",
      "124885/124885 [==============================] - 31s - loss: 0.8642 - acc: 0.6450 - val_loss: 0.7773 - val_acc: 0.6838\n",
      "Epoch 5/5\n",
      "124885/124885 [==============================] - 31s - loss: 0.8365 - acc: 0.6570 - val_loss: 0.7639 - val_acc: 0.6935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a3fcae890>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, labels_train, validation_data=(x_test, labels_test), nb_epoch=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124885 samples, validate on 31175 samples\n",
      "Epoch 1/2\n",
      "124885/124885 [==============================] - 31s - loss: 0.8132 - acc: 0.6687 - val_loss: 0.7347 - val_acc: 0.6963\n",
      "Epoch 2/2\n",
      "124885/124885 [==============================] - 31s - loss: 0.7949 - acc: 0.6763 - val_loss: 0.7127 - val_acc: 0.7099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a941eab90>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-4\n",
    "model.fit(x_train, labels_train, validation_data=(x_test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/2\n",
      "124848/124848 [==============================] - 31s - loss: 0.7634 - acc: 0.6916 - val_loss: 0.8630 - val_acc: 0.6568\n",
      "Epoch 2/2\n",
      "124848/124848 [==============================] - 32s - loss: 0.7470 - acc: 0.6984 - val_loss: 0.8786 - val_acc: 0.6585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a8c738b10>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train first layer\n",
    "model.layers[0].trainable = True\n",
    "model.optimizer.lr = 1e-5\n",
    "model.fit(x_train, labels_train, validation_data=(x_test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'conv1-627pct.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PhraseId  SentenceId                                             Phrase\n",
      "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
      "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
      "2    156063        8545                                                 An\n",
      "3    156064        8545  intermittently pleasing but mostly routine effort\n",
      "4    156065        8545         intermittently pleasing but mostly routine\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(path+'test.tsv', sep='\\t')\n",
    "print(test_df.head())\n",
    "\n",
    "# test_phrases = train_df['Phrase'][:10]\n",
    "test_phrases = test_df['Phrase']\n",
    "norm_test_phrases = map(str.lower, test_phrases.tolist())\n",
    "\n",
    "test_idx_sentencelist = map_sentence2idx(norm_test_phrases, word2idx)\n",
    "test_word_idx = norm_idx(test_idx_sentencelist, vocab_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 55)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_word_idx).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(np.array(test_word_idx).shape)\n",
    "# predictions = model.predict_classes(np.array(test_word_idx), batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# def batchify(sentences, batch_size):\n",
    "#     itertools.islice(batch, batch_size)\n",
    "\n",
    "\n",
    "def split_every(n, iterable):\n",
    "    i = iter(iterable)\n",
    "    piece = list(itertools.islice(i, n))\n",
    "    while piece:\n",
    "        yield np.array(piece)\n",
    "        piece = list(itertools.islice(i, n))\n",
    "        \n",
    "def predict_with_progress(model, sentences, batch_size):\n",
    "    num_samples = sentences.shape[0]\n",
    "    batches = split_every(batch_size, sentences)\n",
    "    p_results = np.zeros((num_samples,)+model.output_shape[1:])\n",
    "    current_index = 0\n",
    "    # Iterative loop\n",
    "    for batch in tqdm(batches, total=math.ceil(num_samples/batch_size)):\n",
    "        if batch is None:\n",
    "            break\n",
    "        if type(batch) is tuple:\n",
    "            batch = batch[0]\n",
    "        p = model.predict_on_batch(batch)\n",
    "        p_size = p.shape[0]\n",
    "#         print('Predictions: {}\\n Size: {}'.format(p_true, p_size))\n",
    "        new_index = current_index + p_size\n",
    "#         print('Current index: {} New index: {} PResults: {}'.format(current_index, new_index, p_results))\n",
    "        p_results[current_index:new_index] = p\n",
    "        current_index = new_index\n",
    "        if current_index >= num_samples:\n",
    "            break\n",
    "    return p_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = predict_with_progress(model, np.array(test_word_idx), batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_predictions = predictions.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66292,), (66292,))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cat_predictions.shape, test_df.PhraseId.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  Sentiment\n",
       "0    156061          3\n",
       "1    156062          3\n",
       "2    156063          2\n",
       "3    156064          3\n",
       "4    156065          3"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = pd.DataFrame({'PhraseId': test_df.PhraseId, 'Sentiment': cat_predictions})\n",
    "# agg = agg[agg.columns[::-1]]\n",
    "agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg.to_csv(path+'submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1099/|/ 99%|| 1099/1105.0 [00:25<00:00, 43.87it/s]"
     ]
    }
   ],
   "source": [
    "!kg submit {path+'submission2.csv'} -c 'sentiment-analysis-on-movie-reviews'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_x_train = np.concatenate([x_train, np.array(test_word_idx)])\n",
    "ps_labels_train = np.concatenate([labels_train, onehot(cat_predictions)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124885, 55)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train.shape, np.array(test_word_idx).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((124885, 5), (66292, 5))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(labels_train.shape, onehot(cat_predictions).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 191177 samples, validate on 31175 samples\n",
      "Epoch 1/1\n",
      "191177/191177 [==============================] - 47s - loss: 0.6718 - acc: 0.7465 - val_loss: 0.7192 - val_acc: 0.7097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0a934f9390>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ps_x_train, ps_labels_train, validation_data=(x_test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "55541a4cbc9a463bb91c1ce98ac3adc1": {
     "views": [
      {
       "cell_index": 68
      }
     ]
    },
    "c1243ba0f60c45f3b7609aedda5e4dee": {
     "views": [
      {
       "cell_index": 64
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
